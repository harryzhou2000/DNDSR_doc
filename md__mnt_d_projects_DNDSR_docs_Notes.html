<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DNDSR: Notes</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">DNDSR
   &#160;<span id="projectnumber">0.0</span>
   </div>
   <div id="projectbrief">Distributed Numeric Data Structure for CFV</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md__mnt_d_projects_DNDSR_docs_Notes.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Notes </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md17"></a>
on periodic boundary</h1>
<p>for simplicity, the single-block mesh supports 1-to-1 definition of periodic bc, which is, the "main" and "donor" are the same bnd meshes given the rigid body transformation.</p>
<p>periodic donor and main must be non-adjacent (not sharing nodes) (main-donor node mapping each pair is not identical) [<em>reasoning: if periodicity needs this, means singularity point, replace this point with a small face with sym condition</em>]</p>
<p>then, record cell2nodePi, and face2nodePi, to augment cell2node ..., and:</p>
<p><code>nodePbi</code> (periodic bits info): bit1-bit2-bit3, means [if peri1][if peri2][if peri3], stored in uint8_t now; the bits show if the node (for the elem) is transferred from a periodic group. if 1-1-1, then p-&gt;transBack_1-&gt;transBack_2-&gt;transBack_3-&gt;p_current, and to obtain the geometry-well coordinate p, use the reverse transformation. see Geom::UnstructuredMesh::Deduplicate1to1Periodic() and Geom::PeriodicInfo::GetCoordByBits().</p>
<p>in the sense of peri-duplicated (original geometry) mesh, node in a cell is different when both cell2node(iCell,ic2n) and cell2nodePI(iCell,ic2n) are different; so use both cell2node and cell2nodePi for inner-cell coord calculation</p>
<p>in the sense of de-duplicated mesh, cell2node(iCell,ic2n) is unique, and points to a coord;</p>
<p>to get interpolate face, the faces are the same when both are true:</p>
<ul>
<li>face2node are the same,</li>
<li>face2nodePi are collaborating: the xor_s of (face2nodePbiL_if2n, face2nodePbiR_if2n) are the same</li>
</ul>
<p>face2nodePi are got from the first cell, like face2node faceAtr are got from the first cell, (for de-duplicated faces, could be periodic-donor or main)</p>
<p>to query face-coords in cell, check is is periodic, if donor, face2cell[1] needs the face to trans-back; if main, face2cell[1] needs the face to trans; face2cell[0] is always good</p>
<p>to query other cell-coord through face, if face is periodic main, if cell is face2cell[0] then other cell needs trans back; cell is face2cell[1] then other cell needs trans</p>
<h1><a class="anchor" id="autotoc_md18"></a>
Point communication race:</h1>
<p>Point-to-point comm in MPI could potentially <b>cause comm race</b> when the comm pattern (of a collective-like comm call) is very dense (close to a all_to_all), while the machine topology is sparse (like in a supercomputer).</p>
<p>calling like:</p>
<div class="fragment"><div class="line">MPI_Request* sendReqs, recvReqs;</div>
<div class="line"><span class="comment">//traversing all ranks in comm, which is the densest, actual code would omit those with zero size</span></div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> rankOther = 0; rankOther &lt; commSize; rankOther++) </div>
<div class="line">    MPI_SendInit(buf,count,...,rankOher,..., sendReqs + rankOther);</div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> rankOther = 0; recvOther &lt; commSize; rankOther++)</div>
<div class="line">    MPI_RecvInit(buf,count,...,rankdOher,..., recvReqs + rankOther); </div>
<div class="line">MPI_Startall(commSize, sendReqs);</div>
<div class="line">MPI_Startall(commSize, recvReqs);</div>
<div class="line"> </div>
<div class="line">MPI_Waitall(commSize, sendReqs, ...);</div>
<div class="line">MPI_Waitall(commSize, recvReqs, ...);</div>
</div><!-- fragment --><p>would launch multiple data transferring tasks in MPI simultaneously, which is ok on a dense machine like a NUMA machine, but it causes network racing and saturation in a sparse machine like a common multi-node supercomputer. The best practice, for the performance of communication, would be like a MPI_Alltoall, which refers to the current machine's topology. On the other hand, alltoall is always global and brings extra zero-sized-data communication overhead globally, which is O(np) overhead. So, <a class="el" href="namespaceDNDS.html">DNDS</a> assumes that the comm pattern is sparse enough, and when the the sparsity exceeds that of the machine, the current implementation of <a class="el" href="namespaceDNDS.html">DNDS</a> uses one-by-one send/recv, like:</p>
<div class="fragment"><div class="line">MPI_Request* sendReqs, recvReqs;</div>
<div class="line"><span class="comment">//traversing all ranks in comm, which is the densest, actual code would omit those with zero size</span></div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> rankOther = 0; rankOther &lt; commSize; rankOther++) </div>
<div class="line">    MPI_SendInit(buf,count,...,rankOher,..., sendReqs + rankOther);</div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> rankOther = 0; recvOther &lt; commSize; rankOther++)</div>
<div class="line">    MPI_RecvInit(buf,count,...,rankdOher,..., recvReqs + rankOther); </div>
<div class="line">MPI_Startall(commSize, sendReqs);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// foreach_recvReqs: start,wait</span></div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> rankOther = 0; recvOther &lt; commSize; rankOther++)</div>
<div class="line">{</div>
<div class="line">    MPI_Start(recvReq[rankOther]);</div>
<div class="line">    MPI_Wait(recvReq[rankOther]);</div>
<div class="line">}</div>
<div class="line"><span class="comment">// </span></div>
<div class="line">MPI_Waitall(commSize, sendReqs, ...);</div>
</div><!-- fragment --><p>which is receiving one-by-one. Sending one-by-one is also ok for solving network race.</p>
<p>Actually, chunk-by-chunk should be used on a sparse machine, with the knowledge of local network capacity. But since sparse decomposed 3-d meshes generally have very small communication overhead, optimization on this is delayed.</p>
<h2><a class="anchor" id="autotoc_md19"></a>
Dynamic-Reforming: Loss of Performance</h2>
<p>For DLR case used for eulerSA3D in <code>d082620525d1d9a07889c9e8c1a9bede70ebe236</code>, on GS machine, times is: 2.1582/it</p>
<p>If using nVarsFixed = -1 (original 6), time is: 3.3944/1t</p>
<h2><a class="anchor" id="autotoc_md20"></a>
Note on HM3:</h2>
<p>must use residual instead of uinc for convergence monitoring!!!</p>
<h2><a class="anchor" id="autotoc_md21"></a>
HM3 testing:</h2>
<p>237m14.899s for hm3 run on HZAU 121m49.267s for ESDIRK4 90m18.275s for BDF2 (3x)</p>
<h1><a class="anchor" id="autotoc_md22"></a>
on running with bssct:</h1>
<p>ompi 3.x seems ungood (module load mpi/openmpi/3.1.6-gcc-9.3.0), for large mesh partition at least; MPI_Barrier even takes 1 sec each!</p>
<p>note that when switching mpi (or other libs), best check entries in cmake, (safest: restart vscode or in standalone terminal), could be unchanged</p>
<h2><a class="anchor" id="autotoc_md23"></a>
curious performance issue on bssct</h2>
<p>with the std::pow in faceFlux function, step time for CylinderA1_L's vorstreet case, is **~5s**, reduces to **~0.2s** at <em>ts 80 (internal 40x80)</em> with std::pow replaced with std::sqrt, step time starts as **~1.3s**, reduces to **~0.2s** at <em>ts 80 (internal 40x80)</em> with std::pow not replaced, but using intel icpc, step time starts as **~0.4s** with replaced with std::sqrt and using intel icpc, step time starts as **~0.4s**</p>
<h1><a class="anchor" id="autotoc_md24"></a>
building hdf5 and cgns</h1>
<div class="fragment"><div class="line">../configure   --enable-fortran=yes --enable-parallel=yes --with-zlib=yes --prefix=$(realpath ../../install) </div>
</div><!-- fragment --><div class="fragment"><div class="line">(only in path: src)</div>
<div class="line">export CC=mpicc</div>
<div class="line">export CXX=mpicxx</div>
<div class="line">export LIBS=&quot;-ldl&quot;</div>
<div class="line">export CPPFLAGS=&quot;-I/usr/include/tk&quot;</div>
<div class="line">export CFLAGS=&quot;-I/usr/include/tk&quot;</div>
<div class="line"> </div>
<div class="line">./configure --prefix=$(realpath ../../install) --with-hdf5=$(realpath ../../install) --enable-parallel --enable-lfs --enable-64bit   --enable-cgnstools=no</div>
</div><!-- fragment --><div class="fragment"><div class="line">make config prefix=$(realpath ../install) cc=gcc</div>
</div><!-- fragment --><div class="fragment"><div class="line">make config prefix=$(realpath ../install) cc=mpicc cxx=mpicxx</div>
</div><!-- fragment --><p>using a v110 api to enable the use by cgns 4.3.0 ? not working? </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1 </li>
  </ul>
</div>
</body>
</html>
